import httpx
import logging
from typing import AsyncGenerator
from models import ChatRequest, LLMRequest, LLMMessage, LLMResponse
from config import get_settings

logger = logging.getLogger(__name__)


class LLMService:
    """Service for communicating with LLM via ngrok"""
    
    def __init__(self):
        self.settings = get_settings()
    
    def _build_system_prompt(self, request: ChatRequest) -> str:
        """Build the system prompt from request context and trigger"""
        
        # Build context section
        context_lines = "\n".join([f"- {item}" for item in request.context])
        
        system_prompt = f"""['Description of a Soccer Game']
ì¶•êµ¬ëŠ” ë‘ íŒ€ì´ ê³µì„ ìƒëŒ€ ê³¨ë¬¸ì— ë„£ì–´ ë“ì í•˜ëŠ” ê²½ê¸°ë‹¤.
ê²½ê¸° ì‹œê°„ì€ ì „ë°˜ 45ë¶„ + í›„ë°˜ 45ë¶„(ì´ 90ë¶„). í•„ìš” ì‹œ ì¶”ê°€ì‹œê°„(Added Time)ì´ ë¶€ì—¬ë  ìˆ˜ ìžˆë‹¤.
í•œ íŒ€ì€ ë³´í†µ 11ëª…(ê³¨í‚¤í¼ í¬í•¨)ìœ¼ë¡œ êµ¬ì„±ëœë‹¤.

['Current Soccer Game Situation']
**context**
{context_lines}

**trigger**
{request.trigger}

['ê³¼ì œ(Task)']
í˜„ìž¬ ì§„í–‰ ì¤‘ì¸ ê²½ê¸°ì— ëŒ€í•´ Aì™€ B ë‘ ì‚¬ëžŒì´ ì´ì•¼ê¸°í•˜ê³  ìžˆë‹¤. ë‘ ì‚¬ëžŒ ëª¨ë‘ {request.selected_team}ì˜ íŒ¬ì´ë©°, Bì˜ ê°ì • í‘œí˜„ì€ ë§¤ìš° ì ê·¹ì ì´ë‹¤(High arousal).
ìœ„ì˜ ì¶•êµ¬ ê²½ê¸° ì„¤ëª…ê³¼ í˜„ìž¬ ìƒí™©ì„ ë°”íƒ•ìœ¼ë¡œ, Person Bì˜ ëŒ€í™” ì‘ë‹µì„ ìƒì„±í•˜ë¼.

['Person Bì˜ í–‰ë™ ê°€ì´ë“œë¼ì¸(Behavioral Guidelines for Person B)']
í–‰ë™ì´ {request.selected_team}ì— ìœ ë¦¬í•˜ë©´: BëŠ” ê·¸ í–‰ë™ì„ ì¢‹ì•„í•œë‹¤.
í–‰ë™ì´ {request.selected_team}ì— ë¶ˆë¦¬í•˜ë©´: BëŠ” ê±°ì¹œ ë¹„ì†ì–´ë¥¼ í‘œí˜„í•œë‹¤.(ìŒìš•ì€ ê¸ˆì§€)
ì»¤ë®¤ë‹ˆí‹°ì²´(ì€ì–´, 'ã…‹/ã…Ž/ã… ,?,!' ë°˜ë³µ)ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
ë¬¸ë²•ì— ì–½ë§¤ì´ì§€ ë§ê³  ì§§ê³  ê°•ë ¬í•˜ê²Œ ë°˜ì‘í•˜ì„¸ìš”.
ì‹¤ì œë¡œ ì±„íŒ…ì„ ì¹˜ëŠ” ê²ƒ ì²˜ëŸ¼ 9ë‹¨ì–´ ì´ë‚´ë¡œ ë¬¸ìž¥ ì„ ì™„ì„±í•˜ì„¸ìš”

fan_team: {request.selected_team}"""
        
        return system_prompt
    
    def _build_llm_request(self, request: ChatRequest) -> LLMRequest:
        """Transform ChatRequest to LLM request format"""
        
        # Build system message
        system_message = LLMMessage(
            role="system",
            content=self._build_system_prompt(request)
        )
        
        # Build chat history messages
        history_messages = [
            LLMMessage(role=msg.role, content=msg.message)
            for msg in request.chat_history
        ]
        
        # Add current user message
        current_message = LLMMessage(role="user", content=request.message)
        
        # Combine all messages
        all_messages = [system_message] + history_messages + [current_message]
        
        return LLMRequest(
            model=self.settings.llm_model,
            messages=all_messages,
            temperature=self.settings.llm_temperature,
            max_tokens=self.settings.llm_max_tokens,
            frequency_penalty=self.settings.llm_frequency_penalty
        )
    
    async def get_chat_completion(self, request: ChatRequest) -> str:
        """Get chat completion from LLM"""
        
        llm_request = self._build_llm_request(request)
        
        # Log request to vLLM
        logger.info("ðŸš€ Sending request to vLLM:")
        logger.info(f"  Endpoint: {self.settings.llm_endpoint}")
        logger.info(f"  Model: {llm_request.model}")
        logger.info(f"  Temperature: {llm_request.temperature}")
        logger.info(f"  Max tokens: {llm_request.max_tokens}")
        logger.info(f"  Frequency penalty: {llm_request.frequency_penalty}")
        logger.info(f"  Messages count: {len(llm_request.messages)}")
        logger.info(f"  Full request payload:")
        logger.info(f"{llm_request.model_dump_json(indent=2)}")
        logger.info("-" * 80)
        
        async with httpx.AsyncClient(timeout=self.settings.llm_timeout) as client:
            response = await client.post(
                self.settings.llm_endpoint,
                json=llm_request.model_dump(),
                headers={
                    "Content-Type": "application/json",
                    "ngrok-skip-browser-warning": "true"
                },
                auth=(self.settings.llm_username, self.settings.llm_password)
            )
            
            # Log response status and body for debugging
            if response.status_code != 200:
                logger.error(f"âŒ vLLM returned error status {response.status_code}")
                logger.error(f"Response body: {response.text}")
            
            response.raise_for_status()
            
            llm_response = LLMResponse(**response.json())
            
            # Extract content from first choice
            content = ""
            if llm_response.choices and len(llm_response.choices) > 0:
                content = llm_response.choices[0].message.content
            
            # Log response from vLLM
            logger.info("âœ… Received response from vLLM:")
            logger.info(f"  Response ID: {llm_response.id}")
            logger.info(f"  Model: {llm_response.model}")
            logger.info(f"  Tokens used: {llm_response.usage.total_tokens}")
            logger.info(f"  Content: {content}")
            if llm_response.choices and len(llm_response.choices) > 0:
                logger.info(f"  Finish reason: {llm_response.choices[0].finish_reason}")
            logger.info(f"  Full response:")
            logger.info(f"{llm_response.model_dump_json(indent=2)}")
            logger.info("=" * 80)
            
            return content
    
    async def stream_chat_completion(self, request: ChatRequest) -> AsyncGenerator[str, None]:
        """Stream chat completion character by character"""
        
        # Get the full response first
        content = await self.get_chat_completion(request)
        
        # Stream it character by character
        for char in content:
            yield char
